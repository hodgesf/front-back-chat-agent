{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2096d49c-d3dc-4329-ada7-aff56d210198",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "llama3_70B = \"nvidia/Llama3-ChatQA-1.5-70B\"\n",
    "llama3_8B = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "phi = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "qwen = \"Qwen/Qwen1.5-72B-Chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b008df98-8394-49da-8fb8-aefe2c90d03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8ff1a64d9f45c695bbea16d6596e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9697a25aef45b3bfb4eb64fe2e800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ce4cf7424b49d58396fbf3452bb374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c9f38eb9e7468cba61453e5c167d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = llama3_8B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map = \"auto\", trust_remote_code=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens = 1000)\n",
    "llama_hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696f3bf-7a6c-47be-91dc-bcfd99d6460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        embeddings = GPT4AllEmbeddings.embed_query()\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398ed46-5b02-4465-b193-a0e70c5c9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port=8000)\n",
    "chroma_client.list_collections()\n",
    "# collection = chroma_client.get_collection(name=\"rare-hack-vd-SC-GPT4ALL-256-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(   \n",
    "    template=\"\"\"<s>[INST]<<SYS>>You are an assistant for answering questions. Only respond to queries about Ehlers-Danlos Syndrome and Hypophosphotasia. \n",
    "    If the question is about anything else, please say that it is out of your capabilities. \n",
    "    Use the following pieces of retrieved context from medical research papers and verified text to answer the question. \n",
    "    You will also be provided with retrieval source \n",
    "    metadata along with the context. Show the URL at the end of your response as a source for your output. \n",
    "    Use three sentences maximum. Do not mention the context in the response.<</SYS>>\n",
    "    Question: {question} \n",
    "    Context: {context}[/INST]\"\"\",\n",
    "    #Metadata: {metadata}[/INST]\"\"\",\n",
    "    input_variables=[\"question\" \"context\"]#, \"metadata\"],\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llama_hf | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What is Ehlers-Danlos Syndrome?\"\n",
    "# retrieval_results = retriever.invoke(question, k = 3)\n",
    "retrieval_results = vd_collection.query(GPT4AllEmbeddings().embed_query(question), n_results = 3)\n",
    "retrieval_results\n",
    "# docs =  retrieval_results[\"documents\"][0]\n",
    "# metadata = retrieval_results[\"metadatas\"][0]\n",
    "# generation = rag_chain.invoke({\"context\": format_docs(retrieval_results), \"question\": question})#, \"metadata\": metadata})\n",
    "# generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaa2b8-865e-48f3-8f64-5e4a95eaffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval_grader(query, retrieval):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c9b09-f6e2-492f-86a2-fdf7552a4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieval Grader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<s>[INST]<<SYS>> You are a grader assessing relevance  of a retrieved document to a user question. \n",
    "    If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score'. Do not explain anything else!<</SYS>>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    JSON: [/INST]\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llama_hf | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2859d-9e5c-491a-9f99-40ba6b1c39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is adams oliver syndrome?\"\n",
    "docs = vectorstore.as_retriever().invoke(question, k=3)\n",
    "\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": docs}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f21594-00d4-48a8-ae2e-4e55a010b540",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Graph Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4b9e4-3ba8-47d6-958c-e5a7112ac6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13043b0f-17c7-49d3-9ea7-8f2c0f0c8691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733ab80-7e7b-4b1a-9519-9242de647eda",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/8d449b67-6bc4-4ecf-9153-759cd21df24f/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcec3e-a09a-40b4-9c15-fead97bf4e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Who are the Bears expected to draft first in the NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46445bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d74d784-daf2-469a-883a-697a98b48dbd",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/c785f9c0-f519-4a38-ad5a-febb59a2139c/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da64a95-736b-4373-8fb4-6ed4bf60a647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c1dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
