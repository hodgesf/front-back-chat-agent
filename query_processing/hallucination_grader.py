from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

def check_hallucination(query, generated_responses, reranked_documents):
    # Perform your hallucination checking logic here
   

    prompt = PromptTemplate(
        template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are an assistant for evaluating the accuracy of responses generated by a chatbot about two rare diseases, Hypophosphatasia and Ehler-Danlos Syndrome. 
        You have access to a database of medical documents related to the question they ask for context and only have the ability to assess responses based on the context provided.
        Use the following pieces of retrieved context to evaluate the chatbot's response. 
        If the response is consistent with the provided context, output 'yes'. If the response is not supported by the context, output 'no'.
        <|eot_id|><|start_header_id|>user<|end_header_id|>
        Question: {question} 
        Context: {context} 
        Chatbot Response: {response}
        Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
        input_variables=["question", "context", "response"],
)
    llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)

    hallucination_grader_chain = prompt | llm | StrOutputParser()

    is_hallucinating = hallucination_grader_chain.invoke({"question": query, "context": reranked_documents, "response": generated_responses})

    return True if is_hallucinating == "yes" else False

